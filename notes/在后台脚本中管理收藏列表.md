# 在后台脚本中管理收藏列表

一开始我把数据存储在 localStorage 里，每个标签页读取并在数据变化时写入。

但是这存在问题，多个标签页之间可能重复请求关注列表，也可能产生数据不一致的问题。

我需要一个全局的管理系统，这只能用后台脚本来做。

## 要解决的问题

非全局时面临的问题：

### 重复请求关注列表

如果两个标签页在几乎同时开始检查收藏数量，并且收藏数量变化了需要全量更新，那么它们都会请求关注数据。

这会造成重复的请求。虽然我已经把检查更新的定时器设置为随机值，但如果用户打开的标签页很多，仍然有可能会几乎同时开始检查，或者是一个页面已经开始获取更新数据，但是另一个页面不知道它的状态，它只能自己也开始请求，造成重复请求。

还有一种情况是当没有存储的关注数据时，用户打开了多个 pixiv 页面，由于没有关注数据，这些页面都会请求关注数据，这也很浪费。

如果要添加一个更新状态，表示已经有页面正在更新数据（让其他页面不要再去请求数据），或者是现在空闲可以更新，那么这相比于读写 localStorage，更适合交给后台来做。

### 数据不一致

假设在某个标签页里，用户添加了一个关注，此页面的数据里就添加了一个关注。

在另一个标签页里，用户添加了另一个关注，这个页面里的数据也添加了一个关注。

现在这两份数据就不一样了。如果它们都把自己的数据写入 localStorage，那么等于后写入的生效，先写入的被覆盖了。

一个解决思路是在合适的时机读取 localStorage 里的数据进行更新，例如一个标签里的数据变化并写入 localStorage 后，其他页面就全都重新从 localStorage 读取数据，更新自己的数据。

但是这难以实现，因为各个页面（或者说所有的 pixiv 标签页）之间缺乏通讯手段，这就不得不借助后台脚本了。

而且还有个数据冲突的情况无法解决：当一个页面 A 开始发送请求获取更新的收藏数据的过程中，注意这个操作不是瞬间完成的。这时候如果其他页面 B 瞬时更新某一个用户数据，B 的数据瞬时写入 localStorage。

那么之后 A 在请求更新数据完成时，这个被删除的用户可能还存在于 A 的数据里（因为用户添加或取消关注某个用户时，A 可能已经获取了一部分数据，而这些数据里含有或缺少这个用户的数据）。由于 A 比 B 后完成，A 的数据随后写入 localStorage 覆盖了数据，这就可能导致 B 的操作失效，并且数据也是不准确的。

因此当 A 在请求期间，其他瞬时完成的操作（添加或取消关注一个用户）不应该写入存储里，而是应该有某种排队等待机制，等待 A 完成操作后，基于 A 的数据进行对应的处理，最后在排队任务为空时才能写入存储。

这无疑也适合使用后台来做，如果用 localStorage 实现这样的排队机制，会是灾难性的，因为各个页面是各自为政，而这个机制需要读写任务队列，还需要配合其他标记（如是否处于更新中的标记）来实现。

## 实现

### 流程

扩展安装或更新时，把初始数据存储到 chrome.storage.local 里。

当用户打开前台 Pixiv 页面时，前台页面向后台请求数据，后台返回数据。

当某个前台页面需要更新数据时，先向后台发送申请，后台会根据情况判断，如果没有其他正在获取更新的页面，则通知（允许）此页面执行更新，并将其任命为更新操作员，之后始终由这个页面发送网络请求来获取新的收藏列表。
如果不符合条件，则不通知（不允许）此页面执行更新。

前台页面的数据变化时（例如获取了全新数据，或者是要添加一个关注、取消一个关注），不可直接操作数据，而是需要向后台提交申请（包含这次操作的详细信息），由后台处理。

当某个页面正在请求新的关注列表时（这是个持续的过程），后台会锁定数据，其他操作请求会被放入等待队列。等待更新任务完成，再执行等待队列里的操作。

### 其他

在前台向后台发送的所有数据里（包括申请进行更新时），都要附带当前登录用户的 ID，后台每次操作也需要根据这个 ID 去操作对应的数据。

其实在申请更新时一般没必要附带用户 ID，包括无痕模式下也是如此，因为无痕模式和普通模式里，虽然用户可能登录两个不同的 pixiv 账号，但是下载器的后台脚本也会分别运行，不会搞混。

但是如果用户在某种模式（如普通模式里），打开了多个 pixiv 页面，然后在有的页面退出登录，切换到新的账号，那么就同时存在两个账号的 pixiv 页面。这时候新账号的页面请求进行更新，后台就需要使用新账号的页面执行更新，而不能使用旧账号的 pixiv 页面。所以此时需要用户 ID 区分它们所属的标签页 ID。

------------

实际上完美的设计是所有操作都分用户 ID，也就是所有状态、数据都区分，或者说每收到一次新的用户 ID 就 new 一个管理后台。但是这样一来比较复杂，二来
